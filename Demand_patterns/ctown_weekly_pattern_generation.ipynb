{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import wntr\n",
    "import wntr_utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import shuffle\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a hourly pattern of multipliers (up to 26280 hours long) mixing together n (max 1950) patterns\n",
    "# from the STREaM dataset; it scales within a range (as expected for WNTR/EPANET input)\n",
    "def create_demand_pattern(df_patterns, n_hours=168, n_mix=50, bound = [0.3,0.9], noise_intensity=0.01):\n",
    "    cols =df_patterns.columns.to_list()\n",
    "    shuffle(cols)\n",
    "    df_patterns = df_patterns[cols]\n",
    "    s = MinMaxScaler(bound)\n",
    "    temp=np.squeeze(s.fit_transform(df_patterns.iloc[:n_hours,:n_mix].mean(axis=1).values.reshape(-1,1)))\n",
    "    dem_pat = temp + np.random.randn(n_hours)*noise_intensity\n",
    "    return dem_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load STREaM data (STREaM is a simulator of end-uses and water demand patterns)\n",
    "# This dataset contains 3 years of hourly (26280 data points) synthetic demand data \n",
    "# for 4 user_classes = ['low','average','high','morning','night']. There are 500 user for each class,\n",
    "# with the first 10 users having leaks. We need to remove these users for our purposes. \n",
    "\n",
    "all_data_path = './data/allData.txt'\n",
    "df = pd.read_csv(all_data_path, header=None)\n",
    "# \n",
    "# create headers\n",
    "user_classes = ['low','average','high','morning','night']\n",
    "headers = []\n",
    "for user_class in user_classes:\n",
    "    suffixes = (('_'+user_class+' ')*400).split() # original\n",
    "    for suffix, number in zip(suffixes,range(1,401)):\n",
    "        headers.append('user'+suffix+'_'+'%03d'%number)  \n",
    "        \n",
    "# reverse df and change df columns\n",
    "df = df.T\n",
    "df.columns = headers\n",
    "\n",
    "# remove first 10 of each class (these have leaks, so not useful for our purposes)\n",
    "users_to_remove = np.array(headers).reshape(-1,10)[::40].ravel().tolist()\n",
    "df.drop(users_to_remove, axis = 1, inplace = True)\n",
    "headers = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load EPANET model\n",
    "inp_file = 'ctown_map_with_controls.inp'\n",
    "out_file_pattern = 'second_batch'\n",
    "topo_name = 'ctown'\n",
    "wn = wntr.network.WaterNetworkModel(inp_file)\n",
    "\n",
    "# save original patterns\n",
    "names, patterns =[],[]\n",
    "for name, pat in wn.patterns():\n",
    "    names.append(name)\n",
    "    patterns.append(pat.multipliers)\n",
    "df_pat_orig = pd.DataFrame(data = patterns).T\n",
    "df_pat_orig.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 3-year pattern for the District Metered Area (DMA)\n",
    "names, patterns =[],[]\n",
    "for name, pat in wn.patterns():\n",
    "    names.append(name)\n",
    "    temp = create_demand_pattern(df, n_hours=24*365*3, n_mix=150)\n",
    "    pat.multipliers = temp\n",
    "    patterns.append(pat.multipliers)\n",
    "df_pat_new = pd.DataFrame(data = patterns).T\n",
    "df_pat_new.columns = names\n",
    "output_total_demands_path = out_file_pattern + '_' + topo_name + '_three_year_demands.csv'\n",
    "df_pat_new.to_csv(output_total_demands_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for pat_name in names:\n",
    "    dd = plt.subplot(len(names),1,i)\n",
    "    plt.plot(df_pat_new[pat_name], label=\"Demand\")\n",
    "    plt.title(pat_name)\n",
    "    plt.subplots_adjust(top=2.00, right=3.00)\n",
    "    plt.xlim(left=0, right=1000)\n",
    "    plt.grid(True)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe with columns = each DMA pattern and 38 random initial pattern values\n",
    "df_pat_new = pd.read_csv(output_total_demands_path)\n",
    "values=[]\n",
    "weekly_simulation_duration = 168\n",
    "ten_days_simulatino_duration = 240\n",
    "weekly=False\n",
    "\n",
    "if weekly:\n",
    "    limit = weekly_simulation_duration\n",
    "else:\n",
    "    limit = ten_days_simulatino_duration\n",
    "\n",
    "for i in range(0,38):\n",
    "    value = random.randint(0, len(df_pat_new)-limit)        \n",
    "    values.append(value)\n",
    "pattern_random_init_point = pd.DataFrame(data = values)\n",
    "pattern_random_init_point.columns = ['start']\n",
    "print(pattern_random_init_point)\n",
    "initial_points_patch = out_file_pattern + '_' + topo_name + '_demand_starting_points.csv'\n",
    "pattern_random_init_point.to_csv(initial_points_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe with initial tank levels for the 7 C-Town tanks and 38 weeks\n",
    "values=[]\n",
    "tank_names = ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7']\n",
    "#tank_names = ['TANK']\n",
    "for name in tank_names:\n",
    "    aux=[]\n",
    "    for i in range(0,38):\n",
    "        tank_max_value = wn.get_node(name).max_level\n",
    "        min_value = tank_max_value*0.10\n",
    "        max_value = tank_max_value*0.80\n",
    "        value = random.uniform(min_value, max_value)        \n",
    "        aux.append(value)\n",
    "    values.append(aux)\n",
    "tank_initial_levels = pd.DataFrame(data = values).T\n",
    "tank_initial_levels.columns = tank_names\n",
    "print(tank_initial_levels)\n",
    "tank_levels_patch = out_file_pattern + '_' + topo_name + '_tank_initial_conditions.csv'\n",
    "tank_initial_levels.to_csv(tank_levels_patch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
